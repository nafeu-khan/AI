{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Step 1: Load and Explore the Dataset\n",
    "df = pd.read_csv('./essays.csv')\n",
    "print(\"Dataset Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['Essay', 'Overall']]\n",
    "\n",
    "# Step 2: Preprocess the Data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset Class\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, essays, scores, tokenizer, max_len=512):\n",
    "        self.essays = essays\n",
    "        self.scores = scores\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.essays)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        essay = str(self.essays[index])\n",
    "        score = self.scores[index]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            essay,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'score': torch.tensor(score, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Split Dataset\n",
    "train_texts, val_texts, train_scores, val_scores = train_test_split(\n",
    "    df['Essay'], df['Overall'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = EssayDataset(train_texts.tolist(), train_scores.tolist(), tokenizer)\n",
    "val_dataset = EssayDataset(val_texts.tolist(), val_scores.tolist(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Step 3: Build the Model\n",
    "class EssayGradingModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EssayGradingModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.regressor = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        score = self.regressor(cls_output)\n",
    "        return score.squeeze()\n",
    "\n",
    "# Initialize Model, Optimizer, and Loss Function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EssayGradingModel().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Step 4: Train the Model\n",
    "def train(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        scores = batch['score'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs, scores)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "def evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_scores = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            scores = batch['score'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, scores)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_scores.extend(scores.cpu().numpy())\n",
    "    \n",
    "    mse = mean_squared_error(true_scores, predictions)\n",
    "    r2 = r2_score(true_scores, predictions)\n",
    "    return total_loss / len(data_loader), mse, r2\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss, mse, r2 = evaluate(model, val_loader, loss_fn, device)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}')\n",
    "\n",
    "# Step 6: Test the Model\n",
    "test_text = \"The essay provided insightful analysis of the topic with well-structured arguments.\"\n",
    "encoding = tokenizer.encode_plus(test_text, add_special_tokens=True, max_length=512, padding='max_length', return_tensors='pt')\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_score = model(input_ids, attention_mask).item()\n",
    "print(f\"Predicted Score: {predicted_score:.2f}\")\n",
    "\n",
    "# Step 7: Visualize Training Performance\n",
    "losses = {'Epoch': [1, 2, 3], 'Train Loss': [0.3, 0.2, 0.1], 'Val Loss': [0.35, 0.25, 0.15]}\n",
    "loss_df = pd.DataFrame(losses)\n",
    "sns.lineplot(data=loss_df, x='Epoch', y='Train Loss', label='Train Loss')\n",
    "sns.lineplot(data=loss_df, x='Epoch', y='Val Loss', label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
